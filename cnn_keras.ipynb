{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB9dc_3UJ8si",
        "colab_type": "code",
        "outputId": "deed334b-1c37-4d24-c893-30cd55b1a1c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#-----installing ktrain library to employ learning rate schedule-------#\n",
        "!pip3 install ktrain"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ktrain\n",
            "  Downloading https://files.pythonhosted.org/packages/59/64/645252e81cec1743f86c65f81636213af107a96e30c338ad8d9fde38e4fd/ktrain-0.1.8.tar.gz\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.21.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.0.3)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (1.16.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->ktrain) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->ktrain) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->ktrain) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->ktrain) (41.0.1)\n",
            "Building wheels for collected packages: ktrain\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/20/65/9837200c0599c8be93473586d3ecebb6c45fc039537898c2e6\n",
            "Successfully built ktrain\n",
            "Installing collected packages: ktrain\n",
            "Successfully installed ktrain-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZLFJM2LMMYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------importing packages----------------#\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import ktrain\n",
        "from ktrain import vision as vis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#---------loading cifar10 from keras.datasets-----------------#\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x=[0]*60000\n",
        "x[:50001]=x_train\n",
        "x[50000:]=x_test\n",
        "y=[0]*60000\n",
        "y[:50001]=y_train\n",
        "y[50000:]=y_test\n",
        "x=np.array(x)  #Since there is no requirment of test set for implementing stratified K fold, all the images are stored in 'x'\n",
        "y=np.array(y)  #All the labels are in 'y'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u03cU7oJ5RKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0926cf8f-dcc5-4888-df61-477caa20909b"
      },
      "source": [
        "print(x.shape)  #dataset contains 60,000 images of dimensions 32*32*3 (height*width*channels)\n",
        "print(y.shape)  #likewise there are 60,000 labels"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 32, 32, 3)\n",
            "(60000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXIZcYJHKgq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------normalization(z-score)-----------------------#\n",
        "mean = np.mean(x,axis=(0,1,2,3))\n",
        "std = np.std(x,axis=(0,1,2,3))\n",
        "x = (x-mean)/(std+1e-7)\n",
        "\n",
        "\n",
        "#-------------------One Hot Encoding---------------# \n",
        "num_classes = 10\n",
        "y_label = np_utils.to_categorical(y,num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leb6_rlSL_9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2a112e6d-0b28-49e5-d0b5-83c535868af7"
      },
      "source": [
        "#---------plotting first few images-------------#\n",
        "\n",
        "for i in range(9):\n",
        "\tplt.subplot(330 + 1 + i) # define subplot\n",
        "\tplt.imshow(x[i])  # define subplot\n",
        "plt.show()  # define subplot\n",
        "\n",
        "#-----10 classes in cifar10 dataset are ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']----------#"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 12:10:45.751612 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.766178 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.781641 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.794484 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.808433 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.822609 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.837303 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.849856 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "W0619 12:10:45.864458 140306142189440 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAD8CAYAAADOg5fGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoOhCNhL1GAwGIqAvUQNBoOhCPz/EE2ve4SvvgQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-Q0EPycZ2j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#------------------defining the cnn model------------------------#\n",
        "\n",
        "def define_model():\n",
        "  baseMapNum = 32\n",
        "  weight_decay = 1e-4\n",
        "  model = Sequential() #intializing the model\n",
        "  \n",
        "  # 32 conv layers of size 3*3 and padding with values same as edge values, output shape would be 32*32*32  \n",
        "  model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32,32,3))) \n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.2)) #dropout with probability 0.2\n",
        "  \n",
        "  #64 conv layers of size 3*3 and the output shape will be 32*32*64\n",
        "  model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.3))\n",
        "  \n",
        "  #128 conv layers of size 3*3 and output shape will be 32*32*128\n",
        "  model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  return model\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqvWBGrn4J7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "ed5a3ffb-3bd4-4bf8-ce48-ef73355ab99f"
      },
      "source": [
        "#------------------summary of the defined model---------------------#\n",
        "model = define_model()\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfcv4opQZySi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "b8d7c010-5ba6-4ace-f4c0-1a2b94755166"
      },
      "source": [
        "#------wrap model and data in ktrain.learner object----------------#\n",
        "model = define_model()\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "learner = ktrain.get_learner(model, train_data=(x, y_label),val_data = (x, y_label))\n",
        "\n",
        "\n",
        "#---------------we use the Learning Rate Finder in ktrain to find a good initial learning rate.------------#\n",
        "learner.lr_find()\n",
        "learner.lr_plot()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 12:10:51.986640 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0619 12:10:52.013617 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0619 12:10:52.018721 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0619 12:10:52.053021 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0619 12:10:52.054164 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0619 12:10:54.636529 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0619 12:10:54.792135 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0619 12:10:54.800485 140306142189440 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0619 12:10:55.214407 140306142189440 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "simulating training for different learning rates... this may take a few moments...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0619 12:10:56.002864 140306142189440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 29s 477us/step - loss: 3.9611 - acc: 0.1794\n",
            "Epoch 2/2\n",
            "32864/60000 [===============>..............] - ETA: 10s - loss: 2.7919 - acc: 0.3230\n",
            "\n",
            "done.\n",
            "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEOCAYAAACKDawAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysDFK8GV9Sxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1b0f92fa-78cd-4dee-fe15-2d9e7cbc4666"
      },
      "source": [
        "#-----------------using the print_layers function to examine the layers of the created network.-----------------#\n",
        "learner.print_layers()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 (trainable=True) : <keras.layers.convolutional.Conv2D object at 0x7f9ab436db38>\n",
            "1 (trainable=True) : <keras.layers.core.Activation object at 0x7f9ab3b07390>\n",
            "2 (trainable=True) : <keras.layers.normalization.BatchNormalization object at 0x7f9ab3b070f0>\n",
            "3 (trainable=True) : <keras.layers.convolutional.Conv2D object at 0x7f9ab3c2ca90>\n",
            "4 (trainable=True) : <keras.layers.core.Activation object at 0x7f9ab3d16d30>\n",
            "5 (trainable=True) : <keras.layers.normalization.BatchNormalization object at 0x7f9ab3f06e10>\n",
            "6 (trainable=True) : <keras.layers.pooling.MaxPooling2D object at 0x7f9ab3ee5d68>\n",
            "7 (trainable=True) : <keras.layers.core.Dropout object at 0x7f9ab4011400>\n",
            "8 (trainable=True) : <keras.layers.convolutional.Conv2D object at 0x7f9ab3e18208>\n",
            "9 (trainable=True) : <keras.layers.core.Activation object at 0x7f9ab429e2b0>\n",
            "10 (trainable=True) : <keras.layers.normalization.BatchNormalization object at 0x7f9ab429e860>\n",
            "11 (trainable=True) : <keras.layers.convolutional.Conv2D object at 0x7f9ab3ad8780>\n",
            "12 (trainable=True) : <keras.layers.core.Activation object at 0x7f9ab3a85b38>\n",
            "13 (trainable=True) : <keras.layers.normalization.BatchNormalization object at 0x7f9ab3a85f28>\n",
            "14 (trainable=True) : <keras.layers.pooling.MaxPooling2D object at 0x7f9ab3a43898>\n",
            "15 (trainable=True) : <keras.layers.core.Dropout object at 0x7f9ab39d2c18>\n",
            "16 (trainable=True) : <keras.layers.convolutional.Conv2D object at 0x7f9ab39fffd0>\n",
            "17 (trainable=True) : <keras.layers.core.Activation object at 0x7f9ab3906da0>\n",
            "18 (trainable=True) : <keras.layers.normalization.BatchNormalization object at 0x7f9ab3906a20>\n",
            "19 (trainable=True) : <keras.layers.convolutional.Conv2D object at 0x7f9ab38ff5f8>\n",
            "20 (trainable=True) : <keras.layers.core.Activation object at 0x7f9ab38afdd8>\n",
            "21 (trainable=True) : <keras.layers.normalization.BatchNormalization object at 0x7f9ab386d2b0>\n",
            "22 (trainable=True) : <keras.layers.pooling.MaxPooling2D object at 0x7f9ab384fba8>\n",
            "23 (trainable=True) : <keras.layers.core.Dropout object at 0x7f9ab37f54a8>\n",
            "24 (trainable=True) : <keras.layers.core.Flatten object at 0x7f9ab37f56a0>\n",
            "25 (trainable=True) : <keras.layers.core.Dense object at 0x7f9ab378fbe0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLs7sLjzbCEJ",
        "colab_type": "code",
        "outputId": "8eadf9f9-f15d-4ce7-bce2-a8a10c8a6320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5868
        }
      },
      "source": [
        "\n",
        "\n",
        "kfold=StratifiedKFold(n_splits=3,shuffle=True)  #defining 3 folds with random shuffling for using StratifiedKFold\n",
        "kfold_acc=[]  #empty list declaration to store validation accuracy for each fold\n",
        "kfold_loss=[] #list to store validation loss for each fold\n",
        "\n",
        "#---------training with adaptive learning rate-----------#\n",
        "for train,test in kfold.split(x,y):\n",
        "  model = define_model()\n",
        "  model.compile(loss='categorical_crossentropy',      #compiling the model using categorical_crossentropy loss function and adam optimizer \n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])\n",
        "  learner = ktrain.get_learner(model, \n",
        "                             train_data=(x[train], y_label[train]),\n",
        "                             val_data = (x[test], y_label[test]))\n",
        "  learner.autofit(0.005)\n",
        "  loss, acc = learner.model.evaluate(x=x[test], y=y_label[test])\n",
        "  print('final loss:%s, final score:%s' % (loss, acc))\n",
        "  kfold_acc.append(acc)\n",
        "  kfold_loss.append(loss)\n",
        "  \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.005...\n",
            "Train on 40000 samples, validate on 20000 samples\n",
            "Epoch 1/1024\n",
            "  416/40000 [..............................] - ETA: 1:53 - loss: 4.0250 - acc: 0.1466"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.135362). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 21s 525us/step - loss: 2.2627 - acc: 0.3939 - val_loss: 1.5131 - val_acc: 0.5216\n",
            "Epoch 2/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 1.4329 - acc: 0.5385 - val_loss: 1.0991 - val_acc: 0.6523\n",
            "Epoch 3/1024\n",
            "40000/40000 [==============================] - 20s 490us/step - loss: 1.1537 - acc: 0.6297 - val_loss: 0.9835 - val_acc: 0.7111\n",
            "Epoch 4/1024\n",
            "40000/40000 [==============================] - 20s 491us/step - loss: 1.1224 - acc: 0.6639 - val_loss: 0.9494 - val_acc: 0.7370\n",
            "Epoch 5/1024\n",
            "40000/40000 [==============================] - 19s 486us/step - loss: 1.0556 - acc: 0.6992 - val_loss: 0.8761 - val_acc: 0.7642\n",
            "Epoch 6/1024\n",
            "40000/40000 [==============================] - 19s 483us/step - loss: 1.0184 - acc: 0.7193 - val_loss: 0.8615 - val_acc: 0.7794\n",
            "Epoch 7/1024\n",
            "40000/40000 [==============================] - 19s 484us/step - loss: 1.0051 - acc: 0.7310 - val_loss: 0.8703 - val_acc: 0.7840\n",
            "Epoch 8/1024\n",
            "40000/40000 [==============================] - 19s 477us/step - loss: 0.9998 - acc: 0.7454 - val_loss: 0.8620 - val_acc: 0.7936\n",
            "\n",
            "Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
            "Epoch 9/1024\n",
            "40000/40000 [==============================] - 19s 478us/step - loss: 0.8864 - acc: 0.7796 - val_loss: 0.7905 - val_acc: 0.8106\n",
            "Epoch 10/1024\n",
            "40000/40000 [==============================] - 19s 480us/step - loss: 0.8319 - acc: 0.7924 - val_loss: 0.7598 - val_acc: 0.8154\n",
            "Epoch 11/1024\n",
            "40000/40000 [==============================] - 19s 483us/step - loss: 0.8084 - acc: 0.7998 - val_loss: 0.7494 - val_acc: 0.8198\n",
            "Epoch 12/1024\n",
            "40000/40000 [==============================] - 19s 479us/step - loss: 0.7901 - acc: 0.8031 - val_loss: 0.7368 - val_acc: 0.8227\n",
            "Epoch 13/1024\n",
            "40000/40000 [==============================] - 19s 473us/step - loss: 0.7746 - acc: 0.8103 - val_loss: 0.7353 - val_acc: 0.8246\n",
            "Epoch 14/1024\n",
            "40000/40000 [==============================] - 19s 473us/step - loss: 0.7592 - acc: 0.8153 - val_loss: 0.7264 - val_acc: 0.8299\n",
            "Epoch 15/1024\n",
            "40000/40000 [==============================] - 19s 473us/step - loss: 0.7528 - acc: 0.8193 - val_loss: 0.7272 - val_acc: 0.8303\n",
            "Epoch 16/1024\n",
            "40000/40000 [==============================] - 19s 475us/step - loss: 0.7406 - acc: 0.8247 - val_loss: 0.7234 - val_acc: 0.8331\n",
            "Epoch 17/1024\n",
            "40000/40000 [==============================] - 19s 470us/step - loss: 0.7356 - acc: 0.8258 - val_loss: 0.7182 - val_acc: 0.8369\n",
            "Epoch 18/1024\n",
            "40000/40000 [==============================] - 19s 470us/step - loss: 0.7332 - acc: 0.8282 - val_loss: 0.7173 - val_acc: 0.8384\n",
            "Epoch 19/1024\n",
            "40000/40000 [==============================] - 19s 472us/step - loss: 0.7249 - acc: 0.8327 - val_loss: 0.7170 - val_acc: 0.8379\n",
            "Epoch 20/1024\n",
            "40000/40000 [==============================] - 19s 477us/step - loss: 0.7227 - acc: 0.8354 - val_loss: 0.7161 - val_acc: 0.8399\n",
            "Epoch 21/1024\n",
            "40000/40000 [==============================] - 19s 476us/step - loss: 0.7190 - acc: 0.8372 - val_loss: 0.7171 - val_acc: 0.8394\n",
            "Epoch 22/1024\n",
            "40000/40000 [==============================] - 19s 475us/step - loss: 0.7139 - acc: 0.8397 - val_loss: 0.7189 - val_acc: 0.8404\n",
            "\n",
            "Epoch 00022: Reducing Max LR on Plateau: new max lr will be 0.00125 (if not early_stopping).\n",
            "Epoch 23/1024\n",
            "40000/40000 [==============================] - 19s 474us/step - loss: 0.6318 - acc: 0.8642 - val_loss: 0.6878 - val_acc: 0.8497\n",
            "Epoch 24/1024\n",
            "40000/40000 [==============================] - 19s 475us/step - loss: 0.6036 - acc: 0.8738 - val_loss: 0.6711 - val_acc: 0.8527\n",
            "Epoch 25/1024\n",
            "40000/40000 [==============================] - 19s 473us/step - loss: 0.5864 - acc: 0.8739 - val_loss: 0.6644 - val_acc: 0.8532\n",
            "Epoch 26/1024\n",
            "40000/40000 [==============================] - 19s 479us/step - loss: 0.5775 - acc: 0.8750 - val_loss: 0.6624 - val_acc: 0.8533\n",
            "Epoch 27/1024\n",
            "40000/40000 [==============================] - 20s 495us/step - loss: 0.5623 - acc: 0.8790 - val_loss: 0.6535 - val_acc: 0.8554\n",
            "Epoch 28/1024\n",
            "40000/40000 [==============================] - 19s 482us/step - loss: 0.5562 - acc: 0.8793 - val_loss: 0.6557 - val_acc: 0.8518\n",
            "Epoch 29/1024\n",
            "40000/40000 [==============================] - 19s 477us/step - loss: 0.5429 - acc: 0.8819 - val_loss: 0.6570 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00029: Reducing Max LR on Plateau: new max lr will be 0.000625 (if not early_stopping).\n",
            "Epoch 30/1024\n",
            "40000/40000 [==============================] - 19s 476us/step - loss: 0.4900 - acc: 0.9019 - val_loss: 0.6392 - val_acc: 0.8583\n",
            "Epoch 31/1024\n",
            "40000/40000 [==============================] - 19s 476us/step - loss: 0.4764 - acc: 0.9029 - val_loss: 0.6408 - val_acc: 0.8578\n",
            "Epoch 32/1024\n",
            "40000/40000 [==============================] - 19s 478us/step - loss: 0.4590 - acc: 0.9075 - val_loss: 0.6368 - val_acc: 0.8608\n",
            "Epoch 33/1024\n",
            "40000/40000 [==============================] - 19s 479us/step - loss: 0.4568 - acc: 0.9063 - val_loss: 0.6346 - val_acc: 0.8594\n",
            "Epoch 34/1024\n",
            "40000/40000 [==============================] - 19s 474us/step - loss: 0.4456 - acc: 0.9098 - val_loss: 0.6311 - val_acc: 0.8615\n",
            "Epoch 35/1024\n",
            "40000/40000 [==============================] - 19s 472us/step - loss: 0.4342 - acc: 0.9128 - val_loss: 0.6338 - val_acc: 0.8609\n",
            "Epoch 36/1024\n",
            "40000/40000 [==============================] - 19s 475us/step - loss: 0.4267 - acc: 0.9139 - val_loss: 0.6335 - val_acc: 0.8608\n",
            "\n",
            "Epoch 00036: Reducing Max LR on Plateau: new max lr will be 0.0003125 (if not early_stopping).\n",
            "Epoch 37/1024\n",
            "40000/40000 [==============================] - 19s 473us/step - loss: 0.3939 - acc: 0.9234 - val_loss: 0.6286 - val_acc: 0.8618\n",
            "Epoch 38/1024\n",
            "40000/40000 [==============================] - 19s 473us/step - loss: 0.3910 - acc: 0.9248 - val_loss: 0.6288 - val_acc: 0.8639\n",
            "Epoch 39/1024\n",
            "40000/40000 [==============================] - 19s 472us/step - loss: 0.3827 - acc: 0.9271 - val_loss: 0.6312 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00039: Reducing Max LR on Plateau: new max lr will be 0.00015625 (if not early_stopping).\n",
            "Epoch 40/1024\n",
            "40000/40000 [==============================] - 19s 470us/step - loss: 0.3584 - acc: 0.9348 - val_loss: 0.6297 - val_acc: 0.8639\n",
            "Epoch 41/1024\n",
            "40000/40000 [==============================] - 19s 474us/step - loss: 0.3634 - acc: 0.9304 - val_loss: 0.6355 - val_acc: 0.8640\n",
            "\n",
            "Epoch 00041: Reducing Max LR on Plateau: new max lr will be 7.8125e-05 (if not early_stopping).\n",
            "Epoch 42/1024\n",
            "40000/40000 [==============================] - 19s 469us/step - loss: 0.3491 - acc: 0.9385 - val_loss: 0.6308 - val_acc: 0.8663\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00042: early stopping\n",
            "Weights from best epoch have been loaded into model.\n",
            "20000/20000 [==============================] - 2s 102us/step\n",
            "final loss:0.6286024710893631, final score:0.86175\n",
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.005...\n",
            "Train on 40000 samples, validate on 20000 samples\n",
            "Epoch 1/1024\n",
            "  416/40000 [..............................] - ETA: 2:18 - loss: 4.1344 - acc: 0.1587"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.215751). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 21s 536us/step - loss: 2.2018 - acc: 0.3974 - val_loss: 1.5923 - val_acc: 0.5394\n",
            "Epoch 2/1024\n",
            "40000/40000 [==============================] - 21s 515us/step - loss: 1.6270 - acc: 0.5064 - val_loss: 1.2129 - val_acc: 0.6124\n",
            "Epoch 3/1024\n",
            "40000/40000 [==============================] - 21s 516us/step - loss: 1.2712 - acc: 0.5921 - val_loss: 1.0297 - val_acc: 0.6831\n",
            "Epoch 4/1024\n",
            "40000/40000 [==============================] - 21s 514us/step - loss: 1.1498 - acc: 0.6475 - val_loss: 0.9444 - val_acc: 0.7279\n",
            "Epoch 5/1024\n",
            "40000/40000 [==============================] - 21s 514us/step - loss: 1.0836 - acc: 0.6806 - val_loss: 0.9058 - val_acc: 0.7490\n",
            "Epoch 6/1024\n",
            "40000/40000 [==============================] - 21s 514us/step - loss: 1.0560 - acc: 0.7041 - val_loss: 0.9023 - val_acc: 0.7610\n",
            "Epoch 7/1024\n",
            "40000/40000 [==============================] - 21s 513us/step - loss: 1.0345 - acc: 0.7231 - val_loss: 0.8781 - val_acc: 0.7802\n",
            "Epoch 8/1024\n",
            "40000/40000 [==============================] - 20s 490us/step - loss: 1.0295 - acc: 0.7333 - val_loss: 0.8776 - val_acc: 0.7883\n",
            "Epoch 9/1024\n",
            "40000/40000 [==============================] - 21s 517us/step - loss: 1.0070 - acc: 0.7467 - val_loss: 0.8642 - val_acc: 0.7953\n",
            "Epoch 10/1024\n",
            "40000/40000 [==============================] - 20s 490us/step - loss: 1.0008 - acc: 0.7534 - val_loss: 0.8508 - val_acc: 0.8040\n",
            "Epoch 11/1024\n",
            "40000/40000 [==============================] - 20s 493us/step - loss: 0.9916 - acc: 0.7594 - val_loss: 0.8502 - val_acc: 0.8055\n",
            "Epoch 12/1024\n",
            "40000/40000 [==============================] - 20s 510us/step - loss: 0.9745 - acc: 0.7654 - val_loss: 0.8403 - val_acc: 0.8121\n",
            "Epoch 13/1024\n",
            "40000/40000 [==============================] - 21s 520us/step - loss: 0.9739 - acc: 0.7695 - val_loss: 0.8461 - val_acc: 0.8127\n",
            "Epoch 14/1024\n",
            "40000/40000 [==============================] - 20s 491us/step - loss: 0.9612 - acc: 0.7750 - val_loss: 0.8360 - val_acc: 0.8155\n",
            "Epoch 15/1024\n",
            "40000/40000 [==============================] - 21s 516us/step - loss: 0.9614 - acc: 0.7762 - val_loss: 0.8363 - val_acc: 0.8155\n",
            "Epoch 16/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 0.9561 - acc: 0.7774 - val_loss: 0.8413 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00016: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
            "Epoch 17/1024\n",
            "40000/40000 [==============================] - 20s 491us/step - loss: 0.8379 - acc: 0.8103 - val_loss: 0.7677 - val_acc: 0.8312\n",
            "Epoch 18/1024\n",
            "40000/40000 [==============================] - 20s 498us/step - loss: 0.7899 - acc: 0.8195 - val_loss: 0.7377 - val_acc: 0.8350\n",
            "Epoch 19/1024\n",
            "40000/40000 [==============================] - 20s 496us/step - loss: 0.7590 - acc: 0.8244 - val_loss: 0.7212 - val_acc: 0.8373\n",
            "Epoch 20/1024\n",
            "40000/40000 [==============================] - 20s 494us/step - loss: 0.7413 - acc: 0.8266 - val_loss: 0.7156 - val_acc: 0.8388\n",
            "Epoch 21/1024\n",
            "40000/40000 [==============================] - 21s 520us/step - loss: 0.7305 - acc: 0.8298 - val_loss: 0.7058 - val_acc: 0.8407\n",
            "Epoch 22/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 0.7162 - acc: 0.8371 - val_loss: 0.7062 - val_acc: 0.8394\n",
            "Epoch 23/1024\n",
            "40000/40000 [==============================] - 20s 510us/step - loss: 0.7147 - acc: 0.8344 - val_loss: 0.7006 - val_acc: 0.8417\n",
            "Epoch 24/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 0.7127 - acc: 0.8352 - val_loss: 0.6949 - val_acc: 0.8434\n",
            "Epoch 25/1024\n",
            "40000/40000 [==============================] - 20s 511us/step - loss: 0.7002 - acc: 0.8391 - val_loss: 0.7010 - val_acc: 0.8436\n",
            "Epoch 26/1024\n",
            "40000/40000 [==============================] - 21s 514us/step - loss: 0.6996 - acc: 0.8391 - val_loss: 0.6983 - val_acc: 0.8430\n",
            "\n",
            "Epoch 00026: Reducing Max LR on Plateau: new max lr will be 0.00125 (if not early_stopping).\n",
            "Epoch 27/1024\n",
            "40000/40000 [==============================] - 20s 511us/step - loss: 0.6302 - acc: 0.8604 - val_loss: 0.6683 - val_acc: 0.8514\n",
            "Epoch 28/1024\n",
            "40000/40000 [==============================] - 21s 520us/step - loss: 0.5994 - acc: 0.8682 - val_loss: 0.6599 - val_acc: 0.8528\n",
            "Epoch 29/1024\n",
            "40000/40000 [==============================] - 20s 496us/step - loss: 0.5768 - acc: 0.8750 - val_loss: 0.6470 - val_acc: 0.8548\n",
            "Epoch 30/1024\n",
            "40000/40000 [==============================] - 20s 493us/step - loss: 0.5649 - acc: 0.8748 - val_loss: 0.6423 - val_acc: 0.8543\n",
            "Epoch 31/1024\n",
            "40000/40000 [==============================] - 19s 486us/step - loss: 0.5533 - acc: 0.8770 - val_loss: 0.6459 - val_acc: 0.8549\n",
            "Epoch 32/1024\n",
            "40000/40000 [==============================] - 20s 488us/step - loss: 0.5405 - acc: 0.8825 - val_loss: 0.6403 - val_acc: 0.8539\n",
            "Epoch 33/1024\n",
            "40000/40000 [==============================] - 20s 491us/step - loss: 0.5375 - acc: 0.8822 - val_loss: 0.6367 - val_acc: 0.8548\n",
            "Epoch 34/1024\n",
            "40000/40000 [==============================] - 20s 495us/step - loss: 0.5355 - acc: 0.8823 - val_loss: 0.6329 - val_acc: 0.8554\n",
            "Epoch 35/1024\n",
            "40000/40000 [==============================] - 20s 494us/step - loss: 0.5211 - acc: 0.8852 - val_loss: 0.6347 - val_acc: 0.8557\n",
            "Epoch 36/1024\n",
            "40000/40000 [==============================] - 20s 493us/step - loss: 0.5271 - acc: 0.8830 - val_loss: 0.6323 - val_acc: 0.8527\n",
            "Epoch 37/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 0.5157 - acc: 0.8874 - val_loss: 0.6291 - val_acc: 0.8558\n",
            "Epoch 38/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 0.5111 - acc: 0.8881 - val_loss: 0.6381 - val_acc: 0.8520\n",
            "Epoch 39/1024\n",
            "40000/40000 [==============================] - 20s 499us/step - loss: 0.5098 - acc: 0.8898 - val_loss: 0.6309 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00039: Reducing Max LR on Plateau: new max lr will be 0.000625 (if not early_stopping).\n",
            "Epoch 40/1024\n",
            "40000/40000 [==============================] - 20s 504us/step - loss: 0.4602 - acc: 0.9038 - val_loss: 0.6305 - val_acc: 0.8562\n",
            "Epoch 41/1024\n",
            "40000/40000 [==============================] - 20s 503us/step - loss: 0.4522 - acc: 0.9061 - val_loss: 0.6286 - val_acc: 0.8563\n",
            "Epoch 42/1024\n",
            "40000/40000 [==============================] - 20s 508us/step - loss: 0.4394 - acc: 0.9095 - val_loss: 0.6224 - val_acc: 0.8582\n",
            "Epoch 43/1024\n",
            "40000/40000 [==============================] - 20s 501us/step - loss: 0.4258 - acc: 0.9118 - val_loss: 0.6284 - val_acc: 0.8574\n",
            "Epoch 44/1024\n",
            "40000/40000 [==============================] - 20s 504us/step - loss: 0.4205 - acc: 0.9128 - val_loss: 0.6258 - val_acc: 0.8573\n",
            "\n",
            "Epoch 00044: Reducing Max LR on Plateau: new max lr will be 0.0003125 (if not early_stopping).\n",
            "Epoch 45/1024\n",
            "40000/40000 [==============================] - 20s 505us/step - loss: 0.3895 - acc: 0.9237 - val_loss: 0.6209 - val_acc: 0.8602\n",
            "Epoch 46/1024\n",
            "40000/40000 [==============================] - 20s 508us/step - loss: 0.3888 - acc: 0.9231 - val_loss: 0.6223 - val_acc: 0.8609\n",
            "Epoch 47/1024\n",
            "40000/40000 [==============================] - 20s 494us/step - loss: 0.3801 - acc: 0.9262 - val_loss: 0.6205 - val_acc: 0.8623\n",
            "Epoch 48/1024\n",
            "40000/40000 [==============================] - 20s 494us/step - loss: 0.3750 - acc: 0.9273 - val_loss: 0.6204 - val_acc: 0.8626\n",
            "Epoch 49/1024\n",
            "40000/40000 [==============================] - 20s 493us/step - loss: 0.3712 - acc: 0.9256 - val_loss: 0.6195 - val_acc: 0.8621\n",
            "Epoch 50/1024\n",
            "40000/40000 [==============================] - 20s 489us/step - loss: 0.3643 - acc: 0.9274 - val_loss: 0.6172 - val_acc: 0.8625\n",
            "Epoch 51/1024\n",
            "40000/40000 [==============================] - 20s 488us/step - loss: 0.3629 - acc: 0.9282 - val_loss: 0.6183 - val_acc: 0.8646\n",
            "Epoch 52/1024\n",
            "40000/40000 [==============================] - 20s 491us/step - loss: 0.3552 - acc: 0.9296 - val_loss: 0.6206 - val_acc: 0.8631\n",
            "\n",
            "Epoch 00052: Reducing Max LR on Plateau: new max lr will be 0.00015625 (if not early_stopping).\n",
            "Epoch 53/1024\n",
            "40000/40000 [==============================] - 20s 492us/step - loss: 0.3361 - acc: 0.9360 - val_loss: 0.6198 - val_acc: 0.8627\n",
            "Epoch 54/1024\n",
            "40000/40000 [==============================] - 20s 495us/step - loss: 0.3409 - acc: 0.9342 - val_loss: 0.6184 - val_acc: 0.8641\n",
            "\n",
            "Epoch 00054: Reducing Max LR on Plateau: new max lr will be 7.8125e-05 (if not early_stopping).\n",
            "Epoch 55/1024\n",
            "40000/40000 [==============================] - 20s 496us/step - loss: 0.3225 - acc: 0.9405 - val_loss: 0.6174 - val_acc: 0.8635\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00055: early stopping\n",
            "Weights from best epoch have been loaded into model.\n",
            "20000/20000 [==============================] - 2s 104us/step\n",
            "final loss:0.6171639852523804, final score:0.8625\n",
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.005...\n",
            "Train on 40000 samples, validate on 20000 samples\n",
            "Epoch 1/1024\n",
            "  352/40000 [..............................] - ETA: 3:10 - loss: 4.5819 - acc: 0.1648"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.318692). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 23s 568us/step - loss: 2.1998 - acc: 0.3916 - val_loss: 1.4077 - val_acc: 0.5475\n",
            "Epoch 2/1024\n",
            "40000/40000 [==============================] - 20s 505us/step - loss: 1.4228 - acc: 0.5513 - val_loss: 1.0549 - val_acc: 0.6601\n",
            "Epoch 3/1024\n",
            "40000/40000 [==============================] - 20s 506us/step - loss: 1.1202 - acc: 0.6396 - val_loss: 0.9037 - val_acc: 0.7217\n",
            "Epoch 4/1024\n",
            "40000/40000 [==============================] - 20s 507us/step - loss: 1.0227 - acc: 0.6867 - val_loss: 0.8521 - val_acc: 0.7537\n",
            "Epoch 5/1024\n",
            "40000/40000 [==============================] - 21s 513us/step - loss: 0.9916 - acc: 0.7123 - val_loss: 0.8490 - val_acc: 0.7702\n",
            "Epoch 6/1024\n",
            "40000/40000 [==============================] - 20s 505us/step - loss: 0.9912 - acc: 0.7301 - val_loss: 0.8463 - val_acc: 0.7847\n",
            "Epoch 7/1024\n",
            "40000/40000 [==============================] - 20s 496us/step - loss: 0.9858 - acc: 0.7433 - val_loss: 0.8545 - val_acc: 0.7931\n",
            "Epoch 8/1024\n",
            "40000/40000 [==============================] - 21s 523us/step - loss: 0.9849 - acc: 0.7545 - val_loss: 0.8582 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
            "Epoch 9/1024\n",
            "40000/40000 [==============================] - 21s 520us/step - loss: 0.8613 - acc: 0.7913 - val_loss: 0.7803 - val_acc: 0.8155\n",
            "Epoch 10/1024\n",
            "40000/40000 [==============================] - 20s 495us/step - loss: 0.8170 - acc: 0.8005 - val_loss: 0.7592 - val_acc: 0.8194\n",
            "Epoch 11/1024\n",
            "40000/40000 [==============================] - 20s 500us/step - loss: 0.7879 - acc: 0.8084 - val_loss: 0.7440 - val_acc: 0.8236\n",
            "Epoch 12/1024\n",
            "40000/40000 [==============================] - 21s 527us/step - loss: 0.7708 - acc: 0.8121 - val_loss: 0.7319 - val_acc: 0.8304\n",
            "Epoch 13/1024\n",
            "40000/40000 [==============================] - 20s 500us/step - loss: 0.7561 - acc: 0.8183 - val_loss: 0.7247 - val_acc: 0.8318\n",
            "Epoch 14/1024\n",
            "40000/40000 [==============================] - 20s 498us/step - loss: 0.7494 - acc: 0.8209 - val_loss: 0.7195 - val_acc: 0.8354\n",
            "Epoch 15/1024\n",
            "40000/40000 [==============================] - 20s 501us/step - loss: 0.7353 - acc: 0.8259 - val_loss: 0.7227 - val_acc: 0.8367\n",
            "Epoch 16/1024\n",
            "40000/40000 [==============================] - 21s 525us/step - loss: 0.7358 - acc: 0.8263 - val_loss: 0.7160 - val_acc: 0.8387\n",
            "Epoch 17/1024\n",
            "40000/40000 [==============================] - 21s 522us/step - loss: 0.7243 - acc: 0.8331 - val_loss: 0.7250 - val_acc: 0.8356\n",
            "Epoch 18/1024\n",
            "40000/40000 [==============================] - 21s 524us/step - loss: 0.7208 - acc: 0.8348 - val_loss: 0.7202 - val_acc: 0.8397\n",
            "\n",
            "Epoch 00018: Reducing Max LR on Plateau: new max lr will be 0.00125 (if not early_stopping).\n",
            "Epoch 19/1024\n",
            "40000/40000 [==============================] - 21s 525us/step - loss: 0.6422 - acc: 0.8597 - val_loss: 0.6884 - val_acc: 0.8466\n",
            "Epoch 20/1024\n",
            "40000/40000 [==============================] - 21s 533us/step - loss: 0.6120 - acc: 0.8651 - val_loss: 0.6677 - val_acc: 0.8506\n",
            "Epoch 21/1024\n",
            "40000/40000 [==============================] - 21s 513us/step - loss: 0.5901 - acc: 0.8707 - val_loss: 0.6690 - val_acc: 0.8485\n",
            "Epoch 22/1024\n",
            "40000/40000 [==============================] - 20s 497us/step - loss: 0.5771 - acc: 0.8734 - val_loss: 0.6603 - val_acc: 0.8511\n",
            "Epoch 23/1024\n",
            "40000/40000 [==============================] - 21s 514us/step - loss: 0.5661 - acc: 0.8740 - val_loss: 0.6617 - val_acc: 0.8491\n",
            "Epoch 24/1024\n",
            "40000/40000 [==============================] - 20s 502us/step - loss: 0.5507 - acc: 0.8799 - val_loss: 0.6535 - val_acc: 0.8508\n",
            "Epoch 25/1024\n",
            "40000/40000 [==============================] - 20s 499us/step - loss: 0.5487 - acc: 0.8801 - val_loss: 0.6474 - val_acc: 0.8535\n",
            "Epoch 26/1024\n",
            "40000/40000 [==============================] - 20s 500us/step - loss: 0.5373 - acc: 0.8812 - val_loss: 0.6509 - val_acc: 0.8518\n",
            "Epoch 27/1024\n",
            "40000/40000 [==============================] - 20s 501us/step - loss: 0.5341 - acc: 0.8842 - val_loss: 0.6437 - val_acc: 0.8532\n",
            "Epoch 28/1024\n",
            "40000/40000 [==============================] - 21s 528us/step - loss: 0.5250 - acc: 0.8845 - val_loss: 0.6508 - val_acc: 0.8514\n",
            "Epoch 29/1024\n",
            "40000/40000 [==============================] - 20s 499us/step - loss: 0.5237 - acc: 0.8864 - val_loss: 0.6486 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00029: Reducing Max LR on Plateau: new max lr will be 0.000625 (if not early_stopping).\n",
            "Epoch 30/1024\n",
            "40000/40000 [==============================] - 20s 499us/step - loss: 0.4701 - acc: 0.9037 - val_loss: 0.6421 - val_acc: 0.8559\n",
            "Epoch 31/1024\n",
            "40000/40000 [==============================] - 21s 528us/step - loss: 0.4595 - acc: 0.9051 - val_loss: 0.6387 - val_acc: 0.8584\n",
            "Epoch 32/1024\n",
            "40000/40000 [==============================] - 20s 504us/step - loss: 0.4477 - acc: 0.9079 - val_loss: 0.6362 - val_acc: 0.8601\n",
            "Epoch 33/1024\n",
            "40000/40000 [==============================] - 20s 502us/step - loss: 0.4363 - acc: 0.9115 - val_loss: 0.6295 - val_acc: 0.8609\n",
            "Epoch 34/1024\n",
            "40000/40000 [==============================] - 21s 526us/step - loss: 0.4296 - acc: 0.9131 - val_loss: 0.6396 - val_acc: 0.8586\n",
            "Epoch 35/1024\n",
            "40000/40000 [==============================] - 21s 528us/step - loss: 0.4241 - acc: 0.9140 - val_loss: 0.6352 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00035: Reducing Max LR on Plateau: new max lr will be 0.0003125 (if not early_stopping).\n",
            "Epoch 36/1024\n",
            "40000/40000 [==============================] - 21s 532us/step - loss: 0.3822 - acc: 0.9279 - val_loss: 0.6338 - val_acc: 0.8602\n",
            "Epoch 37/1024\n",
            "40000/40000 [==============================] - 20s 493us/step - loss: 0.3811 - acc: 0.9269 - val_loss: 0.6400 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00037: Reducing Max LR on Plateau: new max lr will be 0.00015625 (if not early_stopping).\n",
            "Epoch 38/1024\n",
            "40000/40000 [==============================] - 20s 488us/step - loss: 0.3551 - acc: 0.9375 - val_loss: 0.6376 - val_acc: 0.8618\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00038: early stopping\n",
            "Weights from best epoch have been loaded into model.\n",
            "20000/20000 [==============================] - 2s 107us/step\n",
            "final loss:0.6295084849357605, final score:0.8609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg2yOgQBbF3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9284fa96-775b-426a-a4e6-81f0c0bfe57c"
      },
      "source": [
        "#print(kfold_acc)\n",
        "#print(kfold_loss)\n",
        "accuracy = np.mean(np.array(kfold_acc))   #final accuracy is the mean of validation accuracy obtained at each fold \n",
        "loss = np.mean(np.array(kfold_loss))         #final loss is obtained by the mean of validation loss obtained at each fold\n",
        "print(\"Final Score:\"\"{0:.2f}\".format(accuracy*100)) \n",
        "print(\"Final loss:\"\"{0:.2f}\".format(loss))   "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Score:86.17\n",
            "Final loss:0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqp6NCFqh_DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}